{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#INSTALADORES"
      ],
      "metadata": {
        "id": "_kiCqVZRcZhm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aDm5TNRY5eY"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard\n",
        "!pip install torchinfo\n",
        "!pip install --upgrade torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "r9ok5rZZcgqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu3FLT0dTXk0",
        "outputId": "bfbfba50-139a-490c-82cb-2729debd5109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Weights and Biases INICIALIZACION"
      ],
      "metadata": {
        "id": "fOBI94ROcO25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "\n",
        "FALTA GENERAR EL PROYECTO MLP-Obligatorio  Y  OBTENER CLAVE\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "! cp \"/content/drive/MyDrive/Colab Notebooks/DeepLearning/Obligatorio/utils.py\" /content"
      ],
      "metadata": {
        "id": "BVIaUD7HPXBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ENTRENAMIENTO BASE"
      ],
      "metadata": {
        "id": "0ApXh6VQc_tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from fastprogress import master_bar, progress_bar\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchinfo\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "hPA9TFLZZoIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_oneHot(label, numberOfClass):\n",
        "  oneHot_label = torch.zeros(label.shape[0],numberOfClass).to(device)\n",
        "  for i in range(label.shape[0]):\n",
        "    oneHot_label[i][label[i]]=1\n",
        "  return oneHot_label\n",
        "\n",
        "def train_step(mlp_model, criterion, optim, input,tabulars, label, batch_size, numberOfClass):\n",
        "    mlp_model.train()\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    label = to_oneHot(label,numberOfClass)\n",
        "    input = img.view(batch_size, -1)\n",
        "    step_loss = train_step(optim, criterion, mlp_model, input, tabulars, label)\n",
        "    optim.zero_grad()\n",
        "    output = mlp_model(input, tabulars)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    step_loss = loss.item()\n",
        "    return step_loss\n",
        "\n",
        "def train_epoch(mlp_model, loader, criterion, optim,mb,numberOfClass):\n",
        "    epoch_loss_accumulated = 0.0\n",
        "    for img, tabulars, label in  progress_bar(loader,parent = mb):\n",
        "      batch_size = img.size(0)\n",
        "      epoch_loss_accumulated += train_step(mlp_model,criterion,optim, img, tabulars, label, batch_size,numberOfClass)\n",
        "    return epoch_loss_accumulated/len(loader)"
      ],
      "metadata": {
        "id": "eLuLVq3YA1rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_epoch(mlp_model, val_loader, criterion, classes = None):\n",
        "    mlp_model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for images, tabulars, labels in val_loader:\n",
        "        all_labels.extend(labels.numpy())  \n",
        "        labels = labels.to(device)\n",
        "        labels = to_oneHot(label,numberOfClass)\n",
        "        predictions = mlp_model(images.to(device), tabulars)\n",
        "        all_predictions.extend(torch.argmax(predictions, dim=1).cpu().numpy())\n",
        "\n",
        "        loss = criterion(predictions, labels)\n",
        "\n",
        "        epoch_loss += loss.item()    \n",
        "\n",
        "    return epoch_loss / len(val_loader), accuracy_score(all_labels, all_predictions) * 100"
      ],
      "metadata": {
        "id": "kjRB0qPKypTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(mlp_model, train_loader, valid_loader, criterion, optim, number_epochs,numberOfClass):\n",
        "  train_history = []\n",
        "  valid_history = []\n",
        "  accuracy_history = []\n",
        "  now = datetime.datetime.now()\n",
        "  date_time = now.strftime(\"%m%d%Y_%H%M%S\")\n",
        "  name = 'runs/'+mlp_model.name+'_'+date_time\n",
        "  tensorBoard_writer = SummaryWriter(name) \n",
        "  mb = master_bar(range(1, number_epochs+1))\n",
        "  for epoch in mb:\n",
        "      start_time = time.time()     \n",
        "      train_loss = train_epoch(mlp_model, train_loader, criterion, optim,mb,numberOfClass)\n",
        "      train_history.append(train_loss)\n",
        "      print(\"Training epoch {} | Loss {:.6f} | Time {:.2f} seconds\"\n",
        "            .format(epoch + 1, train_loss, time.time() - start_time))\n",
        "      \n",
        "      start_time = time.time()\n",
        "      val_loss, acc = validation_epoch(mlp_model, valid_loader, criterion)\n",
        "      valid_history.append(val_loss)\n",
        "      accuracy_history.append(acc)\n",
        "      print(\"Validation epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n",
        "            .format(epoch + 1, valid_loss, acc, time.time() - start_time))\n",
        "      # Se carga en tensorBoard #Loss #Validation en train y val\n",
        "      tensorBoard_writer.add_scalar(tag=\"Train Loss\", scalar_value=train_loss, global_step=epoch)\n",
        "      tensorBoard_writer.add_scalar(tag=\"Validation Loss\", scalar_value=val_loss, global_step=epoch)\n",
        "      tensorBoard_writer.add_scalar(tag=\"Validation Accuracy\", scalar_value=acc, global_step=epoch)\n",
        "  tensorBoard_writer.close()"
      ],
      "metadata": {
        "id": "o2Q_Vlm0UYK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACA NECESITO ARMAR Vector de vectores img, tabulares, label -------------------------------------------------------------------------------------------------------------------------------------\n",
        "def get_dataloaders(train_transf,batch_size):\n",
        "  train_dataset = ImageFolder(\"train_set\",transform=train_transf)\n",
        "  train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True,transform=transforms.ToTensor())\n",
        "# ACA NECESITO ARMAR Vector de vectores img, tabulares, label -------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  BATCH_SIZE = batch_size\n",
        "\n",
        "  # Separamos en train y validation\n",
        "  train_size = int(0.8 * len(train_dataset))\n",
        "  valid_size = len(train_dataset) - train_size\n",
        "\n",
        "  train, validation = torch.utils.data.random_split(train_dataset, [train_size,valid_size])\n",
        "\n",
        "  print(f\"{len(train)} Training images, {len(validation)} Validation images\")\n",
        "\n",
        "  # Podemos usar data loaders como vimos en el pr√°ctico.\n",
        "  train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True,pin_memory=True)\n",
        "  valid_loader = DataLoader(validation, batch_size=BATCH_SIZE, pin_memory=True)\n",
        "  \n",
        "  return train_loader, valid_loader, len(train_dataset.classes)"
      ],
      "metadata": {
        "id": "jqO-VeKccgRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "DTVLeMkfHqji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_Model(nn.Module):\n",
        "  def __init__(self,name=\"MLP_MODEL\", num_classes):\n",
        "    super().__init__()\n",
        "    self.name = name\n",
        "    self.conv1 = nn.Conv2d(3, 64, 4, stride=2, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.conv2 = nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(128)\n",
        "    self.conv3 = nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(256)\n",
        "    self.conv4 = nn.Conv2d(256, 64, 4, stride=2, padding=1, bias=False)\n",
        "    self.bn4 = nn.BatchNorm2d(64)\n",
        "    self.linear1 = nn.Linear(64*16*12+6, 1024)\n",
        "    self.linear2 = nn.Linear(1024, 512)\n",
        "    self.linear3 = nn.Linear(512, 128)\n",
        "    self.linear4 = nn.Linear(128, 64)\n",
        "    self.out = nn.Linear(64, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x,tabulars):\n",
        "    # entrada de 256*192\n",
        "    x = x.view(x.size(0), 1, 28, 28)\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = torch.concat([x,tabulars],-1)\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = F.relu(self.linear3(x))\n",
        "    x = F.relu(self.linear4(x))\n",
        "    x = F.sigmoid(self.out(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "Vjr6zGNfWcwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(MLP_Model())"
      ],
      "metadata": {
        "id": "hp9pNKPObREb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = wandb.config # Config is a variable that holds and saves hyperparameters and inputs\n",
        "\n",
        "config.LR = 2e-4\n",
        "config.epochs = 50\n",
        "config.batch_size = 32\n",
        "config.B = [0.5,0.999]\n",
        "config.info = 'Modelo MLP'"
      ],
      "metadata": {
        "id": "J3Ru6ebWQ0JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos los dataloaders\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize([256,192]),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Creamos el loaders\n",
        "train_loader, val_loader, num_classes = get_dataloaders(transform_iterator,config.batch_size)"
      ],
      "metadata": {
        "id": "fLVRgXZ9wZNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el modelo y el optimizador\n",
        "mlp_model = MLP_Model(\"MLP MODEL\", num_classes).to(device)\n",
        "opt = torch.optim.Adam(mlp_model.parameters(), lr=config.LR,betas=config.B)\n",
        "crit = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "3GeuvA4gYlnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(mlp_model, train_loader, val_loader, crit, optimizer, config.epochs, num_classes)"
      ],
      "metadata": {
        "id": "1qMpWzA5_lSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardado del modelo\n",
        "\n",
        "torch.save(mlp_model.state_dict(),mlp_model.name+\".dat\")"
      ],
      "metadata": {
        "id": "sU3RPUbglQaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs/"
      ],
      "metadata": {
        "id": "tWTpshes5wYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Weights and Biases IMPLEMENTACION"
      ],
      "metadata": {
        "id": "FqJQhl6pVmlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_2():\n",
        "  \n",
        "  train_loader, val_loader, num_classes = get_dataloaders(transform_iterator,config.batch_size)\n",
        "  mlp_model = MLP_Model(\"MLP MODEL\", num_classes).to(device)\n",
        "  opt = torch.optim.Adam(mlp_model.parameters(), lr=config.LR,betas=config.B)\n",
        "  crit = nn.CrossEntropyLoss()\n",
        "  \n",
        "  train_history = []\n",
        "  valid_history = []\n",
        "  accuracy_history = []\n",
        "  now = datetime.datetime.now()\n",
        "  date_time = now.strftime(\"%m%d%Y_%H%M%S\")\n",
        "  name = 'runs/'+mlp_model.name+'_'+date_time\n",
        "  tensorBoard_writer = SummaryWriter(name) \n",
        "  mb = master_bar(range(1, config.epochs+1))\n",
        "  for epoch in mb:\n",
        "      start_time = time.time()     \n",
        "      train_loss = train_epoch(mlp_model, train_loader, crit, opt,mb,num_classes)\n",
        "      train_history.append(train_loss)\n",
        "      print(\"Training epoch {} | Loss {:.6f} | Time {:.2f} seconds\"\n",
        "            .format(epoch + 1, train_loss, time.time() - start_time))\n",
        "      \n",
        "      start_time = time.time()\n",
        "      val_loss, acc = validation_epoch(mlp_model, val_loader, crit)\n",
        "      valid_history.append(val_loss)\n",
        "      accuracy_history.append(acc)\n",
        "      print(\"Validation epoch {} | Loss {:.6f} | Accuracy {:.2f}% | Time {:.2f} seconds\"\n",
        "            .format(epoch + 1, valid_loss, acc, time.time() - start_time))\n",
        "      # Se carga en tensorBoard #Loss #Validation en train y val\n",
        "      tensorBoard_writer.add_scalar(tag=\"Train Loss\", scalar_value=train_loss, global_step=epoch)\n",
        "      tensorBoard_writer.add_scalar(tag=\"Validation Loss\", scalar_value=val_loss, global_step=epoch)\n",
        "      tensorBoard_writer.add_scalar(tag=\"Validation Accuracy\", scalar_value=acc, global_step=epoch)\n",
        "  tensorBoard_writer.close()"
      ],
      "metadata": {
        "id": "RLWEN2JwXC1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_configuration = {\n",
        "    'method': 'grid',         # 'grid', 'hyperopt', 'bayesian'\n",
        "    'metric': {\n",
        "        'name': 'acc',     # 'accuracy'\n",
        "        'goal': 'maximize'      # 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'batch_size': {'values': [32]},\n",
        "        'epochs': {'values': [5,10,20,50,100]},\n",
        "        'B': {'values': [[0.5,0.999]]},\n",
        "        'learning_rate': {'values': [0.0002, 0.0004, 0.0006, 0.0010, 0.010, 0.1]}\n",
        "     }\n",
        "}"
      ],
      "metadata": {
        "id": "xFhwvb7mTmOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_configuration, project=\"MLP-Obligatorio\", entity=\"vainilla\")\n",
        "wandb.agent(sweep_id, function=train_model_2, count=15, project='MLP-Obligatorio')"
      ],
      "metadata": {
        "id": "JupYfev2TnHZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}