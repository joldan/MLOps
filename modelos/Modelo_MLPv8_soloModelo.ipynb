{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW-Anftmb5sp"
      },
      "outputs": [],
      "source": [
        "class MLP_Model(nn.Module):\n",
        "  def __init__(self,name=\"MLP_MODEL\", vocab_size=260, embedding_dim=4, num_classes=4):\n",
        "    super().__init__()\n",
        "    self.name = name\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim)\n",
        "    # Input -> 256*192\n",
        "    self.conv1 = nn.Conv2d(3, 128, 4, stride=2, padding=1, bias=False)\n",
        "    # 256*192 -> conv1(1/2) -> 128*96\n",
        "    self.bn1 = nn.BatchNorm2d(128)\n",
        "    self.conv2 = nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False)\n",
        "    # 128*96 -> conv1(1/2) -> 64*48\n",
        "    self.bn2 = nn.BatchNorm2d(256)\n",
        "    self.conv3 = nn.Conv2d(256, 128, 4, stride=2, padding=1, bias=False)\n",
        "    # 64*48 -> conv1(1/2) -> 32*24\n",
        "    self.bn3 = nn.BatchNorm2d(128)\n",
        "    self.conv4 = nn.Conv2d(128, 32, 4, stride=2, padding=1, bias=False)\n",
        "    # 32*24 -> conv1(1/2) -> 16*12\n",
        "    self.bn4 = nn.BatchNorm2d(32)\n",
        "    self.conv5 = nn.Conv2d(32, 8, 4, stride=2, padding=1, bias=False)\n",
        "    # 16*12 -> conv1(1/2) -> 8*6\n",
        "    self.bn5 = nn.BatchNorm2d(8)\n",
        "    self.conv6 = nn.Conv2d(8, 1, 4, stride=2, padding=1, bias=False)\n",
        "    # 8*6 -> conv1(1/2) -> 4*3\n",
        "    self.bn6 = nn.BatchNorm2d(1)\n",
        "    self.linear1 = nn.Linear(1*4*3+5+(embedding_dim-1), 1024)\n",
        "    self.linear2 = nn.Linear(1024, 512)\n",
        "    self.linear3 = nn.Linear(512, 128)\n",
        "    self.linear4 = nn.Linear(128, 64)\n",
        "    self.out = nn.Linear(64, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x,tabulars,debug = False):\n",
        "    # entrada de 256*192\n",
        "    emb_Location = self.embedding(tabulars[:,4])\n",
        "    # area data scaling\n",
        "    rescalArea = tabulars[:,0:1] /100.0\n",
        "    #plt.imshow(x[0])\n",
        "    x = x.view(x.size(0), 3, 256, 192)\n",
        "    x = torch.round(x).to(torch.float32)\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "    x = F.relu(self.bn5(self.conv5(x)))\n",
        "    x = F.relu(self.bn6(self.conv6(x)))\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = torch.concat([x,rescalArea,tabulars[:,1:4], emb_Location], -1)\n",
        "    if debug:\n",
        "      print(\"Tabulars START:\")\n",
        "      print(x)\n",
        "      print(\"Tabulars END:\")\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = F.relu(self.linear3(x))\n",
        "    x = F.relu(self.linear4(x))\n",
        "    x = self.out(x)\n",
        "    x = F.softmax(x,dim = 1)\n",
        "    return x"
      ]
    }
  ]
}